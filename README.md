# Representation-Geometry-as-a-Diagnostic-for-Out-of-Distribution-Robustness

## Overview

In this article, we conducted a research to investigate how geometric and topological properties of learned representation spaces relate to robustness under distribution shift. Rather than relying solely on predictive performance, we study whether intrinsic structural characteristics of deep embeddings can serve as reliable diagnostics for out-of-distribution (OOD) generalization.

Specifically, we analyze representations learned by modern neural architectures across multiple training checkpoints and corruption benchmarks, and quantify their geometry using graph-based and spectral invariants. Our central hypothesis is that models exhibiting simpler global spectral structure and smoother local geometry in representation space tend to generalize better under distribution shifts.
